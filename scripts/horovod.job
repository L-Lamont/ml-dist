#!/bin/bash

#SBATCH --job-name=horovod-jobname
#SBATCH --partition=example-partition
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --constraint=a-gpu-constraint
#SBATCH --output=/path/to/logdir/%x.o%j
#SBATCH --error=/path/to/logdir/%x.o%j

msg() { printf "$*\n" >&2; }
die() { msg "!!! $*"; exit 1; }
log() { msg "### $*"; }

module purge &> /dev/null
module load slurm
module load openmpi/4.1.3-mlnx-gcc
module load cuda/11.8.0
module load cuda/nccl_2.11.4-1+cuda11.4
module load python/3.10.7
module load gcc/9.2.0

export http_proxy="http://proxy.per.dug.com:3128"
export https_proxy="http://proxy.per.dug.com:3128"
export HTTP_PROXY="http://proxy.per.dug.com:3128"
export HTTPS_PROXY="http://proxy.per.dug.com:3128"

TEST_NAME="$SLURM_JOB_NAME"
TRAIN_SCRIPT_NAME="tensorflow_training.py" # Set which script to use
FRAMEWORK="horovod"

DATA="/path/to/ml-dist/data/$FRAMEWORK"
TRAIN_SCRIPT="/path/to/ml-dist/frameworks/$FRAMEWORK/$TRAIN_SCRIPT_NAME"
OUTPUT="/path/to/ml-dist/output/$TEST_NAME-$(date +%F.%T)"
VENV="/path/to/ml-dist/venv"

log "TEST_NAME: $TEST_NAME"
log "FRAMEWORK: $FRAMEWORK"
log "TRAIN_SCRIPT: $TRAIN_SCRIPT"
log "OUTPUT: $OUTPUT"
log "DATA: $DATA"
log "VENV: $VENV"

export TMPDIR="$TMPDIR_SHM"
export MASTER_ADDR="$(scontrol show hostname "$SLURM_JOB_NODELIST" | head -n 1):29401"
export XLA_FLAGS="--xla_gpu_cuda_data_dir=$CUDA_PATH"

# Basic Checks
[[ -f "$TRAIN_SCRIPT" ]] || die "Can't find $TRAIN_SCRIPT"

[[ -f "$VENV/bin/activate" ]] || die "Can't find activate in $VENV"

source "$VENV/bin/activate"
[[ -z "$VIRTUAL_ENV" ]] && die "Can't activate $VENV"

mkdir $OUTPUT
[[ -d "$OUTPUT" ]] || die "OUTPUT directory not created"
cd $OUTPUT

trainArgs="
    --data-dir=$DATA
    --output-dir=$OUTPUT
"

mpiArgs="
    --np $SLURM_NTASKS
    --bind-to none
    -map-by slot
    -x NCCL_DEBUG=INFO
    -x NCCL_IB_GID_INDEX=3
    -x LD_LIBRARY_PATH
    -x PATH
    -mca pml ob1
    -mca btl ^openib
"

msg "time mpirun $mpiArgs python $TRAIN_SCRIPT $trainArgs"
time mpirun $mpiArgs python $TRAIN_SCRIPT $trainArgs
